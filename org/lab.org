# -*- coding: utf-8 -*-
# -*- mode: org -*-

#+TITLE:  Reproducibility guide for the openstack-lab

#+STARTUP: overview indent inlineimages logdrawer
#+LANGUAGE:  en
#+OPTIONS:   num:nil toc:t \n:nil @:t ::t |:t ^:nil -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+OPTIONS:   email:nil creator:nil timestamp:t
#+TAGS: noexport(n) deprecated(d)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport

# # Default org-mode HTML style
# #+HTML_HEAD: <link rel="stylesheet" title="Standard" href="http://orgmode.org/worg/style/worg.css" type="text/css" />
# # Shiny readthedocs HTML style
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/readtheorg/css/readtheorg.css"/>
#+HTML_HEAD: <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
#+HTML_HEAD: <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/readtheorg/js/readtheorg.js"></script>

# ### By default, all code chunks are being run when exporting. To
# ### avoid this, simply remove the "# " of the next line.
# #+PROPERTY: header-args :eval never-export

* Introduction
This lab aims to offer an easy way to reproduce the experiments presented in the journal [].
Our experiments are based on the deployment of [[https://www.openstack.org/][OpenStack]].

* Experimental setup
** Requirements
- python 3.7+
- git 
To launch the experiments on a local machine:
- [[https://www.vagrantup.com/][Vagrant]] installed with the [[https://github.com/vagrant-libvirt/vagrant-libvirt][libvirt]] plugin
To launch the experiments on Grid 5000:
- a Grid5000 account and to setting up your ssh config appropriately (see [[#g5k][Grid5000 setup]])

The requirements are all contained in the =requirements.txt= file at the root directory of the lab.
The lab code can be found on [[http://duckduckgo.com][Gitlab]].

*** Grid5000 setup
:PROPERTIES:
:CUSTOM_ID: g5k
:END:
- You need to set the `~/.python-grid5000.yaml` file as explaine [[https://discovery.gitlabpages.inria.fr/enoslib/tutorials/grid5000.html#configuration][here]].
- An appropriate SSH configuration is required to access automatically
  the distant nodes on G5K as `root` for the sake of the deployment process. By
  default, the public key sent to the distant nodes is `~/.ssh/id_rsa.pub`. Be
  sure this key exists.
  (it is otherwise recommended to create a set of key specifically for
  grid5000).
 
Next, be sure the file `~/.ssh/config` contains the following parts, adapted with your own Grid 5000 username:
#+BEGIN_SRC 
# Ease the access to the global access machine
Host g5k
    User <your_g5k_username>
    Hostname access.grid5000.fr
    ForwardAgent yes

# Ease the access to site frontends (e.g. rennes.g5k)
Host *.g5k
    User <your_g5k_username>
    ProxyCommand ssh g5k -W "`basename %h .g5k`:%p"
    ForwardAgent yes

# Ease the access to deployed nodes (e.g. paravance-42-kavlan-4.rennes.grid5000.fr)
Host *.grid5000.fr
    User root
    ProxyCommand ssh -A <your_g5k_username>@194.254.60.33 -W "$(basename %h):%p"
    ForwardAgent yes
#+END_SRC

** Getting the code
First step is recovering all our code from the public repository. The code also includes the results we have obtained and used in the paper.
It is possible to reproduce the graphs and gant charts from the results, please see the related section on Reproducing the graphs.
#+NAME: Getting the lab code
#+BEGIN_SRC sh 
git clone https://gitlab.inria.fr/dpertin/lab-openstack-concerto.git && cd lab-openstack-concerto
#+END_SRC

** Starting a virtual environment
It is recommended to run experiments in a virtual environment to keep your python installation clean.
#+BEGIN_SRC sh
virtualenv -p python3 venv
source venv/bin/activate
#+END_SRC

If your Grid5000 ssh key has a passphrase, you can use 
#+BEGIN_SRC sh
eval "$(ssh-agent -s)" && ssh-add
#+END_SRC
before starting the experiments to add the key to your ssh agent and not have to input the passphrase several times over the deployment.

** Install the requirements in the virtual environment
In the lab code repository, you can use pip to install the python requirements for the lab.
#+BEGIN_SRC sh
pip3 install -r requirements.txt
#+END_SRC
We use  [[https://discovery.gitlabpages.inria.fr/enoslib/][EnOSlib]], that allows to organize our experiment workflow and configure our Grid5000 setup.
We also use [[http://execo.gforge.inria.fr/doc/latest-stable/][Execo]] for the benchmark launches.
 
** Machine Topology used in our experiments
To makes these experiments we have used two different topologies depending on the use of a local registry for the openstack images or a shared / remote registry.
***** Topology with a local registry node
#+NAME: Topology local
#+CAPTION: Topology with local registry node
#+BEGIN_SRC ditaa :file topology_local_registry.png
/--------\         /---------\      
| Madeus |-------->| Compute |------=--------+
|  node  |         |  node   |               |
\--------/         \---------/               |
   |                                         v
   |               /---------\          /---------\
   +-------------->| Control |-----=--->| Local   |   
   |               |  node   |          | Registry|
   |               \---------/          | node    |
   |                                    \---------/
   |                                         ^
   |               /---------\               |
   +-------------->| Network |-----=---------+
                   |   node  |
                   \---------/
#+END_SRC
#+RESULTS: Topology for local registry

***** Topology for remote or shared registry
#+NAME: Topology shared
#+BEGIN_SRC ditaa :file topology_remote_registry.png
/--------\         /---------\      
| Madeus |-------->| Compute |
|  node  |         |  node   |
\--------/         \---------/
   |                          
   |               /---------\
   +-------------->| Control |
   |               |  node   |
   |               \---------/
   |                          
   |                          
   |               /---------\
   +-------------->| Network |
                   |   node  |
                   \---------/
#+END_SRC

#+RESULTS: Topology shared

** Node setup
  The details of the machine reservation on grid5000 are set in the =reservation.yaml= file where they can be updated to fit specific needs.
#+NAME: example of reservation.yaml
#+BEGIN_SRC yaml
---
# ############################################### #
# Grid'5000 reservation parameters                #
# ############################################### #

g5k:
  # reservation: "2018-03-12 19:00:01"
  walltime: "04:00:00"
  job_name: mad-openstack
  env_name: debian10-x64-nfs
  #key: "~/.ssh/id_grid5k.pub"
  resources:
    machines:
      - roles:
        - mad-node
      cluster: paravance
      node: 1
      primary_network: int-net
      - roles:
        - disco/registry
      cluster: paravance
      node: 1
      primary_network: int-net
      - roles:
        - openstack
        - control
      cluster: paravance
      node: 1
      primary_network: int-net
      - roles:
        - openstack
        - compute
      cluster: paravance
      node: 1
      primary_network: int-net
      - roles:
        - openstack
          - network
      node: 1
      cluster: paravance
      primary_network: int-net
    networks:
      - id: int-net
      roles: 
        - network_interface
      type: kavlan
      site: rennes
#+END_SRC
This reservation requests five machines, all from the *paravance* cluster, and all on the same network that is defined as *int-net*  in the last part of this reservation section.
We defined specific roles for our machines:
- The *mad-node* is the node responsible for launching the assemblies for the deployment of openstack on the *openstack* nodes
- The *openstack* are the nodes where openstack will be deployed and in our experiment they each have one specific role (*compute*, *control* and *network*), according to openstack deployment usages
- The *disco/registry* is the node that will hold the docker image repository, for the cases when the repository is local, as opposed to remote or cached.

** Mad Workflow
A typical experiment using Mad is the sequence of several phases:

- deploy :: Mad will read the configuration file, get machines from the resource provider and will prepare the next phase
- install-os :: Mad will deploy OpenStack on the machines. This phase relies on Kolla deployment.
- backup :: Mad will backup metrics gathered, logs and configuration files from the experiment.
- destroy :: Mad will release the resources.

The =README.md= file contains more information about the various commands available. This document focuses on offering an easier way to reproduce the experiments presented in the paper and will not go in details over the various options. The =python mad.py= command has a =--help= flag that gives out information over the commands available.

* Benchmarks
** Request the resources for the benchmarks
#+BEGIN_SRC sh
python mad.py deploy -c reservation.yaml -p g5k --bootstrap
#+END_SRC 
where the =-c= option indicates our reservation file, =-p= indicates the requested provier, and the =--bootstrap= flag indicates that it will populate the inventory with the proper information about the machines reserved and transfer all necessary environment values and files to the different nodes.

This step will issue the reservation of the machines to the chosen Grid5000 cluster, and once the machines have been deployed it will populate all the required files for kolla in a directory that will be linked symbolically to the **current** directory. These configuration files are necessary for the OpenStack deployment.

** Launch the benchmarks

#+NAME: Experiment Scenarios in the reservation file
#+BEGIN_SRC yaml
# ############################################### #
# Experiment Scenarios                            #
# ############################################### #

all:
    # Here are defined the parameters related to the Execo bench engine:
    params:
      test_type: ["seq_1t", "dag_2t", "dag_nt4"]
      registry: ["cached", "local", "remote"]

    # Here are defined global parameters for our benchmarks:
    iterations: 10

test:
    # Here are defined the parameters related to the Execo bench engine:
    params:
      test_type: ["seq_1t", "dag_2t", "dag_nt4"]
      registry: ["cached", "local", "remote"]

    # Here are defined global parameters for our benchmarks:
    iterations: 1  

single:
    # Here are defined the parameters related to the Execo bench engine:
    params:
      test_type: ["dag_nt4"]
      registry: ["local"]

    # Here are defined global parameters for our benchmarks:
    iterations: 1
#+END_SRC 
Each benchmark scenario is defined by a name such as *all*, *test*, or *single* in our file. 
Our parameers are the numbers of iterations for each test that we want to go through and the list of the different assemblies we want to try (such as *seq_1t*, *dag_2t*, *dag_nt4*).

To launch the benchmarks, we use the following command:
#+BEGIN_SRC sh
python mad.py bench -c reservation.yaml --p g5k --test all -e results/
#+END_SRC
The =-p= and =-c= options are similar to the launch of the reservation. The =--test= option defines which tests will be launched from the different scenarios described in the **reservation.yaml** file.

* Results
** Recover the benchmark results
#+BEGIN_SRC sh
python mad.py backup
#+END_SRC
The backup command allows to recover the environment configuration from the experiment and the results that are on the reserved nodes, such as the kolla logs and the madeus directory.

** Plot the data from our given results
Follow these instructions to process our results and produce the graphs we have used in the paper:
 
