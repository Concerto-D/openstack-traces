Distributed software deployment is somewhere between the components' development (functional part) and the running state of the distributed software on the infrastructure (management part). As a result, the particularity of a distributed software commissioning is that it needs information both from the components behaviors and from the underlying infrastructure on which each component will be executed. This frontier is often called the DevOps domain where the commissioning fits in. Note that our definition of software commissioning does not include placement and scheduling optimization problems, nor reconfiguration aspects of the distributed software management.

When designing distributed software, first, the code of each component is written by a given \emph{developer} who often also codes a set of \emph{control 	interfaces} to act on the component. Examples of control interfaces could be \texttt{start}, \texttt{stop} or \texttt{update}, for instance. Most of the time such control interfaces take part in the component commissioning. However, they are not sufficient as they only serve the component level, not the infrastructure level, nor the configuration level. Second, configuration files are often used additionally as \emph{configuration interfaces} to get information from system administrators or operators that cannot be known in advance when writing the functional code of the component. Thirdly, components often require a set of packages or libraries to be installed on the host operating system to work properly. Those requirements are directly related to the \emph{infrastructure management} and may be handled through virtualization or on physical nodes by sysadmins.
%
In the end, the commissioning procedure of a single component is a \emph{coordination program} between these three kind of interfaces. Commissioning
procedures are often explained in \texttt{README} files, or in documentation\footnote{\url{https://doc.ubuntu-fr.org/apache2}}. One can note though that the deployment coordination of a single component can be simplified by using deployment scripts, or ready-to-use virtual images, for instance. In such cases, the deployment is automated and written once. Yet, it should be parametric as information related to infrastructures cannot ne known in advance. Furthermore, it is often difficult to find needed scripts and images without them being fully parametric to be re-usable in many cases. For this reason, deployment coordination procedures are still difficult even for a single component.
%
When considering a complete distributed software composed of a set of
components, the picture becomes more complex as it is needed to coordinate
the commissioning procedures of all the components together with their associated
interactions, each component being possibly designed by different developers. For instance, when installing a very basic Apache/MariaDB system,
additional documentation is
required\footnote{\url{https://doc.ubuntu-fr.org/lamp}} to combine the components. Another example could be the commissioning of Spark on top of Yarn\footnote{\url{https://spark.apache.org/docs/latest/running-on-yarn.html}}. %One can note that in this case, virtual images do not necessarily simplify the coordination as configuration parameters and additional configurations are needed, \eg such as virtual network configuration.
%
Such complex coordination is usually the work of a \emph{devops} engineer. With the increasing complexity of distributed software deployment a very significant growth in the number of languages and tools for the devops community have been visible in recent years.

%%% the three important metrics of this contribution
As already mentioned, in this paper, three metrics are considered important regarding the quality of the automation of distributed software commissioning: (1) a clear separation of concerns between the different actors of the commissioning procedure; (2) the efficiency of commissioning procedures in terms of parallelism expressiveness; and (3) the formalization of the solution such that formal properties can be proven on a given commissioning procedure. If both separation of concerns and formalization are clearly important as in any procedure involving multiple expertise and subject to errors, the efficiency of a deployment need to be motivated a bit more.

%%% precision on the efficiency
Indeed, improving the efficiency of distributed software deployments may seem useless as this procedure is executed once and for all. However, this claim does not consider the problem as a whole and needs clarification. %
First, the commissioning procedure of complex distributed software such as OpenStack (Section~\ref{sec:use-cases}) can take from 15 minutes to more than
one hour to finish, thus gaining in speed is significant. Second, a system
administrator does not perform the commissioning process only once. When new
machines or new clusters are installed in their infrastructure, when errors
occur, or when updates are needed, deployments are executed again.
%
Furthermore, in case of Continuous Integration (CI), Continuous Delivery (CD), and Continuous Deployment (CDep) of companies, research or open source projects, software commissioning are executed a lot such that new features can be tested continuously, thus helping in releasing new versions. For instance, by exploring the traces of the OpenStack Continuous Integration platform\footnote{\url{http://logstash.openstack.org}}, we have observed over nine days, from Februrary 19$^th$ to February 27$^th$ 2020, that the Kolla\footnote{\url{https://wiki.openstack.org/wiki/Kolla}} deployment  of OpenStack have been called almost 3000 times. While being irregular this represents an average of 300 runs per day. Moreover, this period were not a release period where we could expect more calls to Kolla. These numbers will be detailed in Section~\ref{sec:use-cases} and are available on the reproducible lab of the paper.
%
Finally, having an efficient deployment procedure is a first step toward efficient reconfiguration (\eg self-adaptive systems) and dynamic infrastructures (\eg edge computing) where disruption time of services must be minimized.

Of course, many different techniques could be studied to improve the efficiency of distributed software deployments, such as for instance: using optimized and adapted system commands in commissioning scripts and programs (\eg \texttt{rsync} instead of many \texttt{scp}); working on the optimization of a given system command (\eg \nix package manager instead of \texttt{apt-get}); if using virtual images or container images, improving the boot time of hypervisors~\cite{nguyen:hal-02172288}; if using \docker images, optimizing the placement of image layers on the network~\cite{darrous:hal-01745405}; exposing more parallelism in commissioning languages~\cite{dicosmo:hal-01233489}. The scope of this paper is to study this last option, in other words how to improve the parallelism expressiveness of commissioning tools and languages.